{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTRxOWLfv3tkZHe66pK63p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/advanced-rag-system-anatomy/blob/main/Core_Advanced_RAG_components.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install requirements"
      ],
      "metadata": {
        "id": "Hz8JZq6Ob8rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "!pip install -qU langchain langchain-community --no-warn-script-location > /dev/null\n",
        "!pip install -qU beautifulsoup4 --no-warn-script-location > /dev/null\n",
        "!pip install -qU faiss-cpu --no-warn-script-location > /dev/null\n",
        "# use the gpu optimized version of FAISS for better performance\n",
        "#!pip install -qU faiss-gpu --no-warn-script-location > /dev/null\n",
        "!pip install -qU chromadb --no-warn-script-location > /dev/null\n",
        "!pip install -qU validators --no-warn-script-location > /dev/null\n",
        "!pip install -qU sentence_transformers typing-extensions==4.8.0 unstructured --no-warn-script-location > /dev/null\n",
        "!pip install -qU gradio==3.48.0 --no-warn-script-location > /dev/null"
      ],
      "metadata": {
        "id": "SXTdFuTvboyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Documents"
      ],
      "metadata": {
        "id": "pETUBgFAk4Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "\n",
        "# List of URLs to scrape\n",
        "urls = [\"https://langchain-doc.readthedocs.io/en/latest\"\n",
        "        \"https://python.langchain.com/docs/get_started\"]\n",
        "\n",
        "# Initialize an empty list to store the documents\n",
        "docs = []\n",
        "# Looping through each URL in the list - this could take some time!\n",
        "for url in urls:\n",
        "  # max_depth set to 2 for demo purpose, should be increased for real scenario results, e.g. at least 5\n",
        "  loader = RecursiveUrlLoader(url=url, max_depth=4, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "  docs.extend(loader.load())\n",
        "print(f'Downloaded a total of {len(docs)} documents')"
      ],
      "metadata": {
        "id": "eVav9lGgk3X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking documents"
      ],
      "metadata": {
        "id": "0iurKj94w1jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,       # The size of each text chunk\n",
        "    chunk_overlap  = 50,    # Overlap between chunks to ensure continuity\n",
        ")\n",
        "\n",
        "# Stage one: read all the docs, split them into chunks.\n",
        "st = time.time() # Start time for performance measurement\n",
        "print('Loading documents ...')\n",
        "\n",
        "# Split each document into chunks using the configured text splitter\n",
        "chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
        "\n",
        "et = time.time() - st # Calculate time taken for splitting\n",
        "print(f'created {len(chunks)} chunks in {et} seconds.')"
      ],
      "metadata": {
        "id": "zSZJQeA_w2B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build VectorStore: Vectorization"
      ],
      "metadata": {
        "id": "oQGtHuTxkmFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores.utils import filter_complex_metadata\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Path for saving the FAISS index\n",
        "FAISS_INDEX_PATH = \"./vectorstore/lc-faiss-multi-mpnet-500\"\n",
        "\n",
        "\n",
        "#Stage two: embed the docs.\n",
        "# use multi-qa-mpnet-base-dot-v1 sentence transformer to convert pieces of text in vectors to store them in the vector store\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "\n",
        "# use the GPU for faster processing\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "# Initialize HuggingFace embeddings with the specified model\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "#    model_kwargs=model_kwargs  # uncomment when using a GPU, like T4 - requires extended RAM!\n",
        "    )\n",
        "\n",
        "print(f'Loading chunks into vector store ...')\n",
        "st = time.time() # Start time for performance measurement\n",
        "\n",
        "# Create a FAISS vector store from the document chunks and save it locally\n",
        "db = FAISS.from_documents(filter_complex_metadata(chunks), embeddings)\n",
        "# persist vectorstore\n",
        "db.save_local(FAISS_INDEX_PATH)\n",
        "\n",
        "et = time.time() - st\n",
        "print(f'Time taken: {et} seconds.')"
      ],
      "metadata": {
        "id": "qu6sDsq6c9fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load LLM"
      ],
      "metadata": {
        "id": "updDdzwj0RdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "CONFIG = load_dotenv(\".env\")\n",
        "\n",
        "# Retrieve the Hugging Face API token from environment variables\n",
        "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# load HF Token\n",
        "HUGGINGFACEHUB_API_TOKEN=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
        "\n",
        "# Load the model from the Hugging Face Hub\n",
        "model_id = HuggingFaceHub(repo_id=\"mistralai/Mistral-7B-Instruct-v0.1\", model_kwargs={\n",
        "    \"temperature\":0.1,\n",
        "    \"max_new_tokens\":1024,\n",
        "    \"repetition_penalty\":1.2,\n",
        "    \"return_full_text\":False\n",
        "    })\n"
      ],
      "metadata": {
        "id": "GlnNrNdbg2E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever"
      ],
      "metadata": {
        "id": "2m3BIm090jtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# vectorestore\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Load and Initialize the vector store as a retriever for the RAG pipeline\n",
        "db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "\n",
        "retriever = db.as_retriever()"
      ],
      "metadata": {
        "id": "jzqPsuds0kSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template and Chat logic"
      ],
      "metadata": {
        "id": "Bld8lOEv0Uq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval chain\n",
        "from langchain.chains import RetrievalQA\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "global qa\n",
        "template = \"\"\"\n",
        "You are the friendly documentation buddy Arti, who helps novice programmers in using LangChain with simple explanations and examples.\\\n",
        "    Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question :\n",
        "------\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "# Create a PromptTemplate object with specified input variables and the defined template\n",
        "prompt = PromptTemplate.from_template(\n",
        "    template=template,\n",
        ")\n",
        "prompt.format(context=\"context\", history=\"history\", question=\"question\")\n",
        "\n",
        "# Create a memory buffer to manage conversation history\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "\n",
        "# Initialize the RetrievalQA object with the specified model,\n",
        "# retriever, and additional configurations\n",
        "qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=True, return_source_documents=True, chain_type_kwargs={\n",
        "    \"verbose\": True,\n",
        "    \"memory\": memory,\n",
        "    \"prompt\": prompt\n",
        "}\n",
        "    )"
      ],
      "metadata": {
        "id": "K255Ldxq0Xg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI - Gradio"
      ],
      "metadata": {
        "id": "pA5d0LL2kObx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history=[]\n",
        "query=\"draft a function to calculate a mxn matrix\"\n",
        "question=query\n",
        "response=qa({\"query\": query, \"history\": history, \"question\": question})\n",
        "print(*response)"
      ],
      "metadata": {
        "id": "bKeoyhXPrQ2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response['result'])"
      ],
      "metadata": {
        "id": "78wRMjjn0cl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Function to add a new input to the chat history\n",
        "def add_text(history, text):\n",
        "  # Append the new text to the history with a placeholder for the response\n",
        "    history = history + [(text, None)]\n",
        "    return history, \"\"\n",
        "\n",
        "# Function representing the bot's response mechanism\n",
        "def bot(history):\n",
        "    response = infer(history[-1][0], history)\n",
        "    history[-1][1] = response['result']\n",
        "    return history\n",
        "\n",
        "# Function to infer the response using the RAG model\n",
        "def infer(question, history):\n",
        "    query =  question\n",
        "    result = qa({\"query\": query, \"history\": history, \"question\": question})\n",
        "    return result\n",
        "\n",
        "# Building the Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column(elem_id=\"col-container\"):\n",
        "        chatbot = gr.Chatbot([], elem_id=\"chatbot\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "\n",
        "        # Create a row for the question input\n",
        "        with gr.Row():\n",
        "            question = gr.Textbox(label=\"Question\", placeholder=\"Type your question and hit Enter \")\n",
        "\n",
        "    # Define the action when the question is submitted\n",
        "    question.submit(add_text, [chatbot, question], [chatbot, question], queue=False).then(\n",
        "        bot, chatbot, chatbot\n",
        "    )\n",
        "\n",
        "    # Define the action for the clear button\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Launch the Gradio demo interface\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "id": "OHVkFa6MkCir"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}