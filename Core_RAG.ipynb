{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHOltTLFgVnvdUeH+NuM/D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/advanced-rag-system-anatomy/blob/main/Core_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install requirements"
      ],
      "metadata": {
        "id": "Hz8JZq6Ob8rt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "!pip install langchain langchain-community --no-warn-script-location > /dev/null\n",
        "!pip install beautifulsoup4 --no-warn-script-location > /dev/null\n",
        "!pip install faiss-gpu --no-warn-script-location > /dev/null\n",
        "!pip install chromadb --no-warn-script-location > /dev/null\n",
        "!pip install validators --no-warn-script-location > /dev/null\n",
        "!pip install sentence_transformers typing-extensions==4.8.0 unstructured --no-warn-script-location > /dev/null\n",
        "!pip install gradio==3.48.0 --no-warn-script-location > /dev/null"
      ],
      "metadata": {
        "id": "SXTdFuTvboyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Documents"
      ],
      "metadata": {
        "id": "pETUBgFAk4Fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "\n",
        "urls = [\"https://langchain-doc.readthedocs.io/en/latest\"]\n",
        "docs = []\n",
        "for url in urls:\n",
        "  loader = RecursiveUrlLoader(url=url, max_depth=5, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "  docs.extend(loader.load())\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "eVav9lGgk3X3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chunking documents"
      ],
      "metadata": {
        "id": "0iurKj94w1jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import time\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap  = 50,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "# Stage one: read all the docs, split them into chunks.\n",
        "st = time.time()\n",
        "print('Loading documents ...')\n",
        "chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
        "et = time.time() - st\n",
        "print(f'Time taken: {et} seconds.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSZJQeA_w2B3",
        "outputId": "09f060cc-e96e-41bb-a7b0-12940fa64821"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading documents ...\n",
            "Time taken: 0.0037827491760253906 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build VectorStore: Vectorization"
      ],
      "metadata": {
        "id": "oQGtHuTxkmFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores.utils import filter_complex_metadata\n",
        "import time\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "\n",
        "FAISS_INDEX_PATH = \"./vectorstore/lc-faiss-multi-mpnet-500\"\n",
        "\n",
        "\n",
        "#Stage two: embed the docs.\n",
        "# use all-mpnet-base-v2 sentence transformer to convert pieces of text in vectors to store them in the vector store\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "#    model_kwargs=model_kwargs\n",
        "    )\n",
        "print(f'Loading chunks into vector store ...')\n",
        "st = time.time()\n",
        "db = FAISS.from_documents(filter_complex_metadata(chunks), embeddings)\n",
        "db.save_local(FAISS_INDEX_PATH)\n",
        "et = time.time() - st\n",
        "print(f'Time taken: {et} seconds.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu6sDsq6c9fg",
        "outputId": "9b7930a5-7563-4de1-ae19-5613c84766db"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading chunks into vector store ...\n",
            "Time taken: 1.559694528579712 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load preprocessed vector db from S3"
      ],
      "metadata": {
        "id": "yk6Lmau4x6Ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 --no-warn-script-location > /dev/null"
      ],
      "metadata": {
        "id": "WM1asoSjyDeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "# access .env file\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "## Chroma DB\n",
        "s3.download_file('rad-rag-demos', 'vectorstores/chroma.sqlite3', './chroma_db/chroma.sqlite3')\n",
        "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "db.get()\n",
        "\n"
      ],
      "metadata": {
        "id": "8NGAFwQbifds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load LLM"
      ],
      "metadata": {
        "id": "updDdzwj0RdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "\n",
        "# load HF Token\n",
        "config = load_dotenv(\".env\")\n",
        "HUGGINGFACEHUB_API_TOKEN=os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
        "# or use variable\n",
        "#HUGGINGFACEHUB_API_TOKEN = \"\"\n",
        "\n",
        "\n",
        "model_id = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\", model_kwargs={\n",
        "    \"temperature\":0.1,\n",
        "    \"max_new_tokens\":1024,\n",
        "    \"repetition_penalty\":1.2,\n",
        "    \"streaming\": True,\n",
        "    \"return_full_text\":True\n",
        "    })\n"
      ],
      "metadata": {
        "id": "GlnNrNdbg2E6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retriever"
      ],
      "metadata": {
        "id": "2m3BIm090jtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# vectorestore\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "embeddings = HuggingFaceHubEmbeddings(repo_id=model_name)\n",
        "\n",
        "\n",
        "## FAISS DB\n",
        "# s3.download_file('rad-rag-demos', 'vectorstores/faiss_db_ray.zip', './chroma_db/faiss_db_ray.zip')\n",
        "# with zipfile.ZipFile('./chroma_db/faiss_db_ray.zip', 'r') as zip_ref:\n",
        "#     zip_ref.extractall('./chroma_db/')\n",
        "\n",
        "# FAISS_INDEX_PATH='./chroma_db/faiss_db_ray'\n",
        "db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "\n",
        "retriever = db.as_retriever(search_type = \"mmr\")#, search_kwargs={'k': 5, 'fetch_k': 25})"
      ],
      "metadata": {
        "id": "jzqPsuds0kSs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Template and Chat logic"
      ],
      "metadata": {
        "id": "Bld8lOEv0Uq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieval chain\n",
        "from langchain.chains import RetrievalQA\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "global qa\n",
        "template = \"\"\"\n",
        "You are the friendly documentation buddy Arti, who helps the Human in using RAY, the open-source unified framework for scaling AI and Python applications.\\\n",
        "    Use the following context (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) to answer the question :\n",
        "------\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "{question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"context\", \"question\"],\n",
        "    template=template,\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=True, return_source_documents=True, chain_type_kwargs={\n",
        "    \"verbose\": True,\n",
        "    \"memory\": memory,\n",
        "    \"prompt\": prompt\n",
        "}\n",
        "    )"
      ],
      "metadata": {
        "id": "K255Ldxq0Xg6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UI - Gradio"
      ],
      "metadata": {
        "id": "pA5d0LL2kObx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import random\n",
        "import time\n",
        "\n",
        "def add_text(history, text):\n",
        "    history = history + [(text, None)]\n",
        "    return history, \"\"\n",
        "\n",
        "def bot(history):\n",
        "    response = infer(history[-1][0], history)\n",
        "    print(*memory)\n",
        "    sources = [doc.metadata.get(\"source\") for doc in response['source_documents']]\n",
        "    src_list = '\\n'.join(sources)\n",
        "    print_this = response['result']+\"\\n\\n\\n Sources: \\n\\n\\n\"+src_list\n",
        "\n",
        "    #history[-1][1] = \"\"\n",
        "    #for character in response['result']: #print_this:\n",
        "    #    history[-1][1] += character\n",
        "    #    time.sleep(0.05)\n",
        "    #    yield history\n",
        "    history[-1][1] = print_this #response['result']\n",
        "    return history\n",
        "\n",
        "def infer(question, history):\n",
        "    query =  question\n",
        "    result = qa({\"query\": query, \"history\": history, \"question\": question})\n",
        "    return result\n",
        "\n",
        "css=\"\"\"\n",
        "#col-container {max-width: 700px; margin-left: auto; margin-right: auto;}\n",
        "\"\"\"\n",
        "\n",
        "title = \"\"\"\n",
        "<div style=\"text-align: center;max-width: 700px;\">\n",
        "    <h1>Chat with your Documentation</h1>\n",
        "    <p style=\"text-align: center;\">Chat with Documentation, <br />\n",
        "    when everything is ready, you can start asking questions about the docu ;)</p>\n",
        "</div>\n",
        "\"\"\"\n",
        "\n",
        "with gr.Blocks(css=css) as demo:\n",
        "    with gr.Column(elem_id=\"col-container\"):\n",
        "        gr.HTML(title)\n",
        "        chatbot = gr.Chatbot([], elem_id=\"chatbot\")\n",
        "        clear = gr.Button(\"Clear\")\n",
        "        with gr.Row():\n",
        "            question = gr.Textbox(label=\"Question\", placeholder=\"Type your question and hit Enter \")\n",
        "    question.submit(add_text, [chatbot, question], [chatbot, question], queue=False).then(\n",
        "        bot, chatbot, chatbot\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        },
        "id": "OHVkFa6MkCir",
        "outputId": "8a52b642-79d0-484e-be8a-9cd87b04f23b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7863, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}