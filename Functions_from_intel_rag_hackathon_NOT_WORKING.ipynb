{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/advanced-rag-system-anatomy/blob/main/Functions_from_intel_rag_hackathon_NOT_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqQEdSLdQRTL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "!pip install -qU langchain langchain-community --no-warn-script-location > /dev/null\n",
        "!pip install -qU beautifulsoup4 --no-warn-script-location > /dev/null\n",
        "!pip install -qU faiss-gpu --no-warn-script-location > /dev/null\n",
        "!pip install -qU chromadb --no-warn-script-location > /dev/null\n",
        "!pip install -qU validators --no-warn-script-location > /dev/null\n",
        "!pip install -qU sentence_transformers typing-extensions==4.8.0 unstructured --no-warn-script-location > /dev/null\n",
        "!pip install -qU gradio==3.48.0 --no-warn-script-location > /dev/null\n",
        "!pip install -qU boto3 --no-warn-script-location > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaTLBRcMQTGZ"
      },
      "outputs": [],
      "source": [
        "# documents loader function\n",
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from validators import url as url_validator\n",
        "\n",
        "def load_docs_from_urls(urls: list = None, max_depth: int = 5):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    if urls is None:\n",
        "        urls = [\"https://docs.python.org/3/\"]  # Default URL list\n",
        "    docs = []\n",
        "    for url in urls:\n",
        "        if not url_validator(url):\n",
        "            raise ValueError(f\"Invalid URL: {url}\")\n",
        "        loader = RecursiveUrlLoader(url=url, max_depth=max_depth, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "        docs.extend(loader.load())\n",
        "    print(f\"loaded {len(docs)} pages\")\n",
        "    return docs\n",
        "    #documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvwGhQWfVr_q"
      },
      "outputs": [],
      "source": [
        "load_docs_from_urls('https://docs.python.org/3/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dMilMCjYQV3v"
      },
      "outputs": [],
      "source": [
        "# embeddings functions\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import time\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "def create_embeddings(\n",
        "        docs: list[Document], \n",
        "        chunk_size:int, \n",
        "        chunk_overlap:int,\n",
        "        embedding_model: str = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\", \n",
        "        ):\n",
        "    \"\"\"given a sequence of `Document` objects this fucntion will\n",
        "    generate embeddings for it.\n",
        "    \n",
        "    ## argument\n",
        "    :params docs (list[Document]) -> list of `list[Document]`\n",
        "    :params chunk_size (int) -> chunk size in which documents are chunks\n",
        "    :params chunk_overlap (int) -> the amount of token that will be overlapped between chunks\n",
        "    :params embedding_model (str) -> the huggingspace model that will embed the documents \n",
        "    ## Return\n",
        "    Tuple of embedding and chunks\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap  = chunk_overlap,\n",
        "        length_function = len,\n",
        "    )\n",
        "\n",
        "    # Stage one: read all the docs, split them into chunks.\n",
        "    st = time.time()\n",
        "    print('Loading documents ...')\n",
        "\n",
        "    chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
        "    et = time.time() - st\n",
        "    print(f'Time taken: {et} seconds.')\n",
        "\n",
        "    #Stage two: embed the docs.\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    print(f\"create a total of {len(chunks)}\")\n",
        "\n",
        "    return embeddings,chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXQZr6aISDTh"
      },
      "outputs": [],
      "source": [
        "# preprocessed vectorstore retrieval\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "import zipfile\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# access .env file\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "#    model_kwargs=model_kwargs\n",
        "    )\n",
        "\n",
        "## FAISS\n",
        "FAISS_INDEX_PATH='./vectorstore/lc-faiss-multi-mpnet-500-markdown'\n",
        "VS_DESTINATION = FAISS_INDEX_PATH+\".zip\"\n",
        "s3.download_file('rad-rag-demos', 'vectorstores/lc-faiss-multi-mpnet-500-markdown.zip', VS_DESTINATION)\n",
        "with zipfile.ZipFile(VS_DESTINATION, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./vectorstore/')\n",
        "faissdb = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "\n",
        "## Chroma DB\n",
        "chroma_directory=\"./vectorstore/lc-chroma-multi-mpnet-500-markdown\"\n",
        "VS_DESTINATION = chroma_directory+\".zip\"\n",
        "s3.download_file('rad-rag-demos', 'vectorstores/lc-chroma-multi-mpnet-500-markdown.zip', VS_DESTINATION)\n",
        "with zipfile.ZipFile(VS_DESTINATION, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./vectorstore/')\n",
        "chromadb = Chroma(persist_directory=chroma_directory, embedding_function=embeddings)\n",
        "chromadb.get()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09zb3AyhR-Ao"
      },
      "outputs": [],
      "source": [
        "# vectorization functions\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import time\n",
        "\n",
        "def build_vector_store(docs: list, db_path: str, embedding_model: str, new_db:bool=False, chunk_size:int=500, chunk_overlap:int=50):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if db_path is None:\n",
        "        FAISS_INDEX_PATH = \"./vectorstore/py-faiss-multi-mpnet-500\"\n",
        "    else:\n",
        "        FAISS_INDEX_PATH = db_path\n",
        "\n",
        "    embeddings,chunks = create_embeddings(docs, embedding_model, chunk_size, chunk_overlap)\n",
        "\n",
        "    #load chunks into vector store\n",
        "    print(f'Loading chunks into faiss vector store ...')\n",
        "    st = time.time()\n",
        "    if new_db:\n",
        "        db_faiss = FAISS.from_documents(chunks, embeddings)\n",
        "    else:\n",
        "        db_faiss = FAISS.add_documents(chunks, embeddings)\n",
        "    db_faiss.save_local(FAISS_INDEX_PATH)\n",
        "    et = time.time() - st\n",
        "    print(f'Time taken: {et} seconds.')\n",
        "\n",
        "    #print(f'Loading chunks into chroma vector store ...')\n",
        "    #st = time.time()\n",
        "    #persist_directory='./vectorstore/py-chroma-multi-mpnet-500'\n",
        "    #db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
        "    #et = time.time() - st\n",
        "    #print(f'Time taken: {et} seconds.')\n",
        "    result = f\"built vectore store at {FAISS_INDEX_PATH}\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps0QIHipQ_rg"
      },
      "outputs": [],
      "source": [
        "# prompts and chatlogic function\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# prediction guard\n",
        "from langchain.llms import PredictionGuard\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "category_template = \"\"\"### Instruction:\n",
        "Read the below input and determine if it is a request to search, explain, generate computer code?\n",
        "Respond only with \"generation\" if it requests code, \"explanation\" if asks for explaination, \"search\" if it is searching for a function or tool and no other text.\n",
        "Respond with \"chat\" if it does not fit any of the mentioned categories and no other text.</s>\n",
        "\n",
        "### Input:\n",
        "{query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "category_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=category_template\n",
        ")\n",
        "\n",
        "qa_template = \"\"\"### Instruction:\n",
        "Read the documentation exerpt (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) below to respond with a detailed answer to the given question.\n",
        "First take a step back to reflect on the question breaking it down into smaller steps and then go through the answer step by step and one function at a time.\n",
        "If the question cannot be answered based on the documentation exerpt alone or the documentation does not explicitly say the answer to the question,\n",
        "write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "<ctx>\n",
        "Documentation: {context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "Question: {query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=qa_template\n",
        ")\n",
        "\n",
        "chat_template = \"\"\"### Instruction:\n",
        "You are a friendly and clever AI assistant. Respond to the latest human message in the conversation below.\n",
        "Use the context (delimited by <ctx></ctx>) and conversation history (delimited by <hs></hs>).\n",
        "\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "Human: {query}</s>\n",
        "AI:\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=chat_template\n",
        ")\n",
        "\n",
        "code_template = \"\"\"### Instruction:\n",
        "You are a code generation assistant. Respond with a code snippet and any explanation requested in the below input.\n",
        "Use the documentation context (delimited by <ctx></ctx>) and conversation history (delimited by <hs></hs>) to better understand the goal of the code\n",
        "\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "{query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "code_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=code_template\n",
        ")\n",
        "\n",
        "\n",
        "def get_response_chain(query, history, question):\n",
        "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "    embeddings = HuggingFaceHubEmbeddings(repo_id=model_name)\n",
        "  # Determine what kind of message this is.\n",
        "    print(f\"asking the Llama!\")\n",
        "    msg_category = pg.Completion.create(\n",
        "      model=\"Nous-Hermes-Llama2-13B\",\n",
        "      prompt=category_prompt.format(query=query)\n",
        "    )['choices'][0]['text'].lower()\n",
        "\n",
        "    print(f\"asked the llama and it said: {msg_category}\")\n",
        "\n",
        "  # configure our chain\n",
        "\n",
        "    if msg_category == \"explanation\":\n",
        "        print(f\"it is an explanation\")\n",
        "\n",
        "        # Handle the informational request.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"WizardCoder\",\n",
        "        #    prompt=qa_prompt.format(context=info_context, history=chat_history, query=message)\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text'].split('#')[0].strip()\n",
        "        model_id = HuggingFaceHub(repo_id=\"codellama/CodeLlama-13b-Instruct-hf\", model_kwargs={\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/lc-faiss-multi-mpnet-500'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"query\")\n",
        "        formatted_prompt = qa_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "\n",
        "    elif msg_category == \"generation\":\n",
        "        print(f\"it is a generation\")\n",
        "\n",
        "        # Handle the code generation request.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"WizardCoder\",\n",
        "        #    prompt=code_prompt.format(context=info_context, history=chat_history, query=message),\n",
        "        #    #max_tokens=500\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text']\n",
        "        model_id = HuggingFaceHub(repo_id=\"codellama/CodeLlama-13b-Instruct-hf\", model_kwargs={\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/lc-git-faiss'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "        formatted_prompt = code_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        print(f\"it is a chat\")\n",
        "\n",
        "        # Handle the chat message.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"Nous-Hermes-Llama2-13B\",\n",
        "        #    prompt=chat_prompt.format(context=info_context, history=chat_history, query=message),\n",
        "        #    output={\n",
        "        #        \"toxicity\": True\n",
        "        #    }\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text'].split('Human:')[0].strip()\n",
        "        model_id = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\", model_kwargs={\n",
        "            \"temperature\":0.1,\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            \"return_full_text\":True\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/py-faiss-multi-mpnet-500'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "        formatted_prompt = chat_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "    return qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnb_5dU1RDUa"
      },
      "outputs": [],
      "source": [
        "# retriever and qa_chain function\n",
        "\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# vectorestore\n",
        "from langchain.vectorstores import FAISS\n",
        "# retrieval chain\n",
        "from langchain.chains import RetrievalQA\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "def get_db_retriever(vector_db:str=None):\n",
        "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "    embeddings = HuggingFaceHubEmbeddings(repo_id=model_name)\n",
        "\n",
        "    #db = Chroma(persist_directory=\"./vectorstore/lc-chroma-multi-mpnet-500\", embedding_function=embeddings)\n",
        "    #db.get()\n",
        "    if not vector_db:\n",
        "        FAISS_INDEX_PATH='./vectorstore/py-faiss-multi-mpnet-500'\n",
        "    else:\n",
        "        FAISS_INDEX_PATH=vector_db\n",
        "    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def qa_chain(query, history, question):\n",
        "\n",
        "    retriever = get_db_retriever()\n",
        "    prompt, model_id = get_response_chain(query, history, question)\n",
        "    print(f\"calling qa_instance with {prompt}\\n, {model_id}\\n, {query}\\n, {history}\\n, {question}\\n\")\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "    qa_instance = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "        #\"verbose\": True,\n",
        "        \"memory\": memory,\n",
        "        \"prompt\": prompt\n",
        "        }\n",
        "    )\n",
        "    result = qa_instance({\"query\": query, \"history\": history, \"question\": question})\n",
        "\n",
        "    return result\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN7iPDjfxPzryAyjA3kBzlg",
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
