{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcR8wZXa0l5wTxchmizv9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/almutareb/advanced-rag-system-anatomy/blob/main/Functions_from_intel_rag_hackathon_NOT_WORKING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqQEdSLdQRTL"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "!pip install langchain langchain-community --no-warn-script-location > /dev/null\n",
        "!pip install beautifulsoup4 --no-warn-script-location > /dev/null\n",
        "!pip install faiss-gpu --no-warn-script-location > /dev/null\n",
        "!pip install chromadb --no-warn-script-location > /dev/null\n",
        "!pip install validators --no-warn-script-location > /dev/null\n",
        "!pip install sentence_transformers typing-extensions==4.8.0 unstructured --no-warn-script-location > /dev/null\n",
        "!pip install gradio==3.48.0 --no-warn-script-location > /dev/null\n",
        "!pip install boto3 --no-warn-script-location > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# documents loader function\n",
        "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
        "from bs4 import BeautifulSoup as Soup\n",
        "from validators import url as url_validator\n",
        "\n",
        "def load_docs_from_urls(urls: list = None, max_depth: int = 5):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    if urls is None:\n",
        "        urls = [\"https://docs.python.org/3/\"]  # Default URL list\n",
        "    docs = []\n",
        "    for url in urls:\n",
        "        if not url_validator(url):\n",
        "            raise ValueError(f\"Invalid URL: {url}\")\n",
        "        loader = RecursiveUrlLoader(url=url, max_depth=max_depth, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
        "        docs.extend(loader.load())\n",
        "    print(f\"loaded {len(docs)} pages\")\n",
        "    return docs\n",
        "    #documents = loader.load()"
      ],
      "metadata": {
        "id": "eaTLBRcMQTGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# embeddings functions\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import time\n",
        "\n",
        "\n",
        "def create_embeddings(docs: list, embedding_model: str, chunk_size:int, chunk_overlap:int):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
        "        chunk_size = chunk_size,\n",
        "        chunk_overlap  = chunk_overlap,\n",
        "        length_function = len,\n",
        "    )\n",
        "\n",
        "    # Stage one: read all the docs, split them into chunks.\n",
        "    st = time.time()\n",
        "    print('Loading documents ...')\n",
        "\n",
        "    chunks = text_splitter.create_documents([doc.page_content for doc in docs], metadatas=[doc.metadata for doc in docs])\n",
        "    et = time.time() - st\n",
        "    print(f'Time taken: {et} seconds.')\n",
        "\n",
        "    #Stage two: embed the docs.\n",
        "    if embedding_model is None:\n",
        "        embedding_model = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    print(f\"create a total of {len(chunks)}\")\n",
        "\n",
        "    return embeddings,chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dMilMCjYQV3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessed vectorstore retrieval\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "import zipfile\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# access .env file\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "## FAISS\n",
        "FILE_NAME = 'lc-faiss-multi-mpnet-500-markdown'\n",
        "FAISS_INDEX_PATH = './vectorstore/'+FILE_NAME\n",
        "VS_DESTINATION = FAISS_INDEX_PATH+\".zip\"\n",
        "s3.download_file('rad-rag-demos', 'vectorstores/lc-faiss-multi-mpnet-500-markdown.zip', VS_DESTINATION)\n",
        "with zipfile.ZipFile(VS_DESTINATION, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./vectorstore/')\n",
        "\n",
        "## Chroma DB\n",
        "VS_DESTINATION = \"./vectorstore/lc-chroma-multi-mpnet-500-markdown.zip\"\n",
        "s3.download_file('rad-rag-demos', 'vectorstore/lc-chroma-multi-mpnet-500-markdown.zip', VS_DESTINATION)\n",
        "#db = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
        "#db.get()\n",
        "with zipfile.ZipFile(VS_DESTINATION, 'r') as zip_ref:\n",
        "    zip_ref.extractall('./vectorstore/')\n",
        "\n",
        "model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "#    model_kwargs=model_kwargs\n",
        "    )\n",
        "\n",
        "db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "lXQZr6aISDTh",
        "outputId": "eee78bf8-3ae2-4cf4-8d61-79c3c1836d0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './vectorstore/lc-faiss-multi-mpnet-500-markdown.zip.656CEC4c'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-807ea8d1e648>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mFAISS_INDEX_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./vectorstore/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFILE_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mVS_DESTINATION\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS_INDEX_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".zip\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0ms3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rad-rag-demos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectorstores/lc-faiss-multi-mpnet-500-markdown.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVS_DESTINATION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVS_DESTINATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./vectorstore/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/s3/inject.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mS3Transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtransfer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return transfer.download_file(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0mbucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/boto3/s3/transfer.py\u001b[0m in \u001b[0;36mdownload_file\u001b[0;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[1;32m    403\u001b[0m         )\n\u001b[1;32m    404\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m             \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;31m# This is for backwards compatibility where when retries are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# exceeded we need to throw the same error from boto3 instead of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# however if a KeyboardInterrupt is raised we want want to exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;31m# out of this and propagate the exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# final result.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# main() method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_and_set_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/tasks.py\u001b[0m in \u001b[0;36m_execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Executing task {self} with kwargs {kwargs_to_display}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0;31m# If the task is the final task, then set the TransferFuture's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# value to the return value from main().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/download.py\u001b[0m in \u001b[0;36m_main\u001b[0;34m(self, fileobj, data, offset)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0moffset\u001b[0m \u001b[0mto\u001b[0m \u001b[0mwrite\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdata\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \"\"\"\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0mfileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mseek\u001b[0;34m(self, where, whence)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36m_open_if_needed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_byte\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fileobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_byte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/s3transfer/utils.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, filename, mode)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './vectorstore/lc-faiss-multi-mpnet-500-markdown.zip.656CEC4c'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorization functions\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import ReadTheDocsLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import time\n",
        "\n",
        "def build_vector_store(docs: list, db_path: str, embedding_model: str, new_db:bool=False, chunk_size:int=500, chunk_overlap:int=50):\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if db_path is None:\n",
        "        FAISS_INDEX_PATH = \"./vectorstore/py-faiss-multi-mpnet-500\"\n",
        "    else:\n",
        "        FAISS_INDEX_PATH = db_path\n",
        "\n",
        "    embeddings,chunks = create_embeddings(docs, embedding_model, chunk_size, chunk_overlap)\n",
        "\n",
        "    #load chunks into vector store\n",
        "    print(f'Loading chunks into faiss vector store ...')\n",
        "    st = time.time()\n",
        "    if new_db:\n",
        "        db_faiss = FAISS.from_documents(chunks, embeddings)\n",
        "    else:\n",
        "        db_faiss = FAISS.add_documents(chunks, embeddings)\n",
        "    db_faiss.save_local(FAISS_INDEX_PATH)\n",
        "    et = time.time() - st\n",
        "    print(f'Time taken: {et} seconds.')\n",
        "\n",
        "    #print(f'Loading chunks into chroma vector store ...')\n",
        "    #st = time.time()\n",
        "    #persist_directory='./vectorstore/py-chroma-multi-mpnet-500'\n",
        "    #db_chroma = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
        "    #et = time.time() - st\n",
        "    #print(f'Time taken: {et} seconds.')\n",
        "    result = f\"built vectore store at {FAISS_INDEX_PATH}\"\n",
        "    return result"
      ],
      "metadata": {
        "id": "09zb3AyhR-Ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompts and chatlogic function\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# prediction guard\n",
        "from langchain.llms import PredictionGuard\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "category_template = \"\"\"### Instruction:\n",
        "Read the below input and determine if it is a request to search, explain, generate computer code?\n",
        "Respond only with \"generation\" if it requests code, \"explanation\" if asks for explaination, \"search\" if it is searching for a function or tool and no other text.\n",
        "Respond with \"chat\" if it does not fit any of the mentioned categories and no other text.</s>\n",
        "\n",
        "### Input:\n",
        "{query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "category_prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=category_template\n",
        ")\n",
        "\n",
        "qa_template = \"\"\"### Instruction:\n",
        "Read the documentation exerpt (delimited by <ctx></ctx>) and the chat history (delimited by <hs></hs>) below to respond with a detailed answer to the given question.\n",
        "First take a step back to reflect on the question breaking it down into smaller steps and then go through the answer step by step and one function at a time.\n",
        "If the question cannot be answered based on the documentation exerpt alone or the documentation does not explicitly say the answer to the question,\n",
        "write \"Sorry I had trouble answering this question, based on the information I found.\"\n",
        "\n",
        "<ctx>\n",
        "Documentation: {context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "Question: {query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=qa_template\n",
        ")\n",
        "\n",
        "chat_template = \"\"\"### Instruction:\n",
        "You are a friendly and clever AI assistant. Respond to the latest human message in the conversation below.\n",
        "Use the context (delimited by <ctx></ctx>) and conversation history (delimited by <hs></hs>).\n",
        "\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "Human: {query}</s>\n",
        "AI:\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=chat_template\n",
        ")\n",
        "\n",
        "code_template = \"\"\"### Instruction:\n",
        "You are a code generation assistant. Respond with a code snippet and any explanation requested in the below input.\n",
        "Use the documentation context (delimited by <ctx></ctx>) and conversation history (delimited by <hs></hs>) to better understand the goal of the code\n",
        "\n",
        "<ctx>\n",
        "{context}\n",
        "</ctx>\n",
        "\n",
        "------\n",
        "<hs>\n",
        "{history}\n",
        "</hs>\n",
        "------\n",
        "</s>\n",
        "\n",
        "### Input:\n",
        "{query}</s>\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "code_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"history\", \"query\"],\n",
        "    template=code_template\n",
        ")\n",
        "\n",
        "\n",
        "def get_response_chain(query, history, question):\n",
        "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "    embeddings = HuggingFaceHubEmbeddings(repo_id=model_name)\n",
        "  # Determine what kind of message this is.\n",
        "    print(f\"asking the Llama!\")\n",
        "    msg_category = pg.Completion.create(\n",
        "      model=\"Nous-Hermes-Llama2-13B\",\n",
        "      prompt=category_prompt.format(query=query)\n",
        "    )['choices'][0]['text'].lower()\n",
        "\n",
        "    print(f\"asked the llama and it said: {msg_category}\")\n",
        "\n",
        "  # configure our chain\n",
        "\n",
        "    if msg_category == \"explanation\":\n",
        "        print(f\"it is an explanation\")\n",
        "\n",
        "        # Handle the informational request.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"WizardCoder\",\n",
        "        #    prompt=qa_prompt.format(context=info_context, history=chat_history, query=message)\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text'].split('#')[0].strip()\n",
        "        model_id = HuggingFaceHub(repo_id=\"codellama/CodeLlama-13b-Instruct-hf\", model_kwargs={\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/lc-faiss-multi-mpnet-500'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"query\")\n",
        "        formatted_prompt = qa_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "\n",
        "    elif msg_category == \"generation\":\n",
        "        print(f\"it is a generation\")\n",
        "\n",
        "        # Handle the code generation request.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"WizardCoder\",\n",
        "        #    prompt=code_prompt.format(context=info_context, history=chat_history, query=message),\n",
        "        #    #max_tokens=500\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text']\n",
        "        model_id = HuggingFaceHub(repo_id=\"codellama/CodeLlama-13b-Instruct-hf\", model_kwargs={\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/lc-git-faiss'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "        formatted_prompt = code_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "    else:\n",
        "        print(f\"it is a chat\")\n",
        "\n",
        "        # Handle the chat message.\n",
        "        #result = pg.Completion.create(\n",
        "        #    model=\"Nous-Hermes-Llama2-13B\",\n",
        "        #    prompt=chat_prompt.format(context=info_context, history=chat_history, query=message),\n",
        "        #    output={\n",
        "        #        \"toxicity\": True\n",
        "        #    }\n",
        "        #)\n",
        "        #completion = result['choices'][0]['text'].split('Human:')[0].strip()\n",
        "        model_id = HuggingFaceHub(repo_id=\"HuggingFaceH4/zephyr-7b-beta\", model_kwargs={\n",
        "            \"temperature\":0.1,\n",
        "            \"max_new_tokens\":2048,\n",
        "            \"repetition_penalty\":1.2,\n",
        "            \"return_full_text\":True\n",
        "            })\n",
        "        FAISS_INDEX_PATH='./vectorstore/py-faiss-multi-mpnet-500'\n",
        "        db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "        retriever = db.as_retriever()\n",
        "        memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "        formatted_prompt = chat_prompt.format(context=db.similarity_search(query),history=history, query=query)\n",
        "        qa = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "            #\"verbose\": True,\n",
        "            \"memory\": memory,\n",
        "            \"prompt\": formatted_prompt\n",
        "        }\n",
        "            )\n",
        "\n",
        "    return qa"
      ],
      "metadata": {
        "id": "ps0QIHipQ_rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retriever and qa_chain function\n",
        "\n",
        "# HF libraries\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain.embeddings import HuggingFaceHubEmbeddings\n",
        "# vectorestore\n",
        "from langchain.vectorstores import FAISS\n",
        "# retrieval chain\n",
        "from langchain.chains import RetrievalQA\n",
        "# prompt template\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "def get_db_retriever(vector_db:str=None):\n",
        "    model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
        "    embeddings = HuggingFaceHubEmbeddings(repo_id=model_name)\n",
        "\n",
        "    #db = Chroma(persist_directory=\"./vectorstore/lc-chroma-multi-mpnet-500\", embedding_function=embeddings)\n",
        "    #db.get()\n",
        "    if not vector_db:\n",
        "        FAISS_INDEX_PATH='./vectorstore/py-faiss-multi-mpnet-500'\n",
        "    else:\n",
        "        FAISS_INDEX_PATH=vector_db\n",
        "    db = FAISS.load_local(FAISS_INDEX_PATH, embeddings)\n",
        "\n",
        "    retriever = db.as_retriever()\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def qa_chain(query, history, question):\n",
        "\n",
        "    retriever = get_db_retriever()\n",
        "    prompt, model_id = get_response_chain(query, history, question)\n",
        "    print(f\"calling qa_instance with {prompt}\\n, {model_id}\\n, {query}\\n, {history}\\n, {question}\\n\")\n",
        "\n",
        "    memory = ConversationBufferMemory(memory_key=\"history\", input_key=\"question\")\n",
        "    qa_instance = RetrievalQA.from_chain_type(llm=model_id, chain_type=\"stuff\", retriever=retriever, verbose=False, return_source_documents=True, chain_type_kwargs={\n",
        "        #\"verbose\": True,\n",
        "        \"memory\": memory,\n",
        "        \"prompt\": prompt\n",
        "        }\n",
        "    )\n",
        "    result = qa_instance({\"query\": query, \"history\": history, \"question\": question})\n",
        "\n",
        "    return result\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "hnb_5dU1RDUa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}